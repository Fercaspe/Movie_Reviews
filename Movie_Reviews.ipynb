{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7NTWcgOM5zkgBniLrTdq+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fercaspe/Movie_Reviews/blob/main/Movie_Reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbPnypGaOva5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_style('darkgrid') # darkgrid, white grid, dark, white and ticks\n",
        "plt.rc('axes', titlesize=18)     # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=14)    # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=13)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=13)    # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=13)    # legend fontsize\n",
        "plt.rc('font', size=13)          # controls default text sizes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Data\n",
        "Goal: Our goal is to find which machine learning model is best suited to predict sentiment (output) given a movie review (input).\n",
        "\n",
        "* Input(x) -> movie review\n",
        "* Ourput(y) -> sentiment\n",
        "\n",
        "#Reading dataset"
      ],
      "metadata": {
        "id": "_ohaC2dIO40J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_review = pd.read_csv('IMDB Dataset.csv')\n",
        "df_review"
      ],
      "metadata": {
        "id": "soY5HnUkPKiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#taking a smaller sample of 10000 rows to make processing faster and get imbalance data\n",
        "# 9000 positives\n",
        "df_positive = df_review[df_review['sentiment']=='positive'][:9000]\n",
        "# 1000 positives\n",
        "df_negative = df_review[df_review['sentiment']=='negative'][:1000]\n",
        "\n",
        "df_review_imb = pd.concat([df_positive, df_negative])\n",
        "df_review_imb.value_counts(['sentiment'])"
      ],
      "metadata": {
        "id": "51P2lXPZPSkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dealing with Imbalanced Classes"
      ],
      "metadata": {
        "id": "LhcJCKgqPXGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colors = sns.color_palette('deep')\n",
        "\n",
        "plt.figure(figsize=(8,4), tight_layout=True)\n",
        "plt.bar(x=['Positive', 'Negative'],\n",
        "        height=df_review_imb.value_counts(['sentiment']),\n",
        "        color=colors[:2])\n",
        "plt.title('Sentiment')\n",
        "plt.savefig('sentiment.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xEY-tlqHPY6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different options to balanced data\n",
        "\n",
        "1.   Python imbalanced-learn module\n",
        "2.   DataFrame.sample\n",
        "\n"
      ],
      "metadata": {
        "id": "gHTnCgHHPog5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import  RandomUnderSampler\n",
        "\n",
        "rus = RandomUnderSampler(random_state=0)\n",
        "df_review_bal, df_review_bal['sentiment']=rus.fit_resample(df_review_imb[['review']],\n",
        "                                                           df_review_imb['sentiment'])\n",
        "df_review_bal\n",
        "\n",
        "# option 2\n",
        "# length_negative = len(df_review[df_review['sentiment']=='negative'])\n",
        "# df_review_positive = df_review[df_review['sentiment']=='positive'].sample(n=length_negative)\n",
        "# df_review_non_positive = df_review[~(df_review['sentiment']=='positive')]\n",
        "\n",
        "# df_review = pd.concat([\n",
        "#     df_review_positive, df_review_non_positive\n",
        "# ])\n",
        "# df_review['sentiment'].value_counts()\n",
        "\n",
        "# df_review.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "2V6ndOZPPxM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_review_imb.value_counts('sentiment'))\n",
        "print(df_review_bal.value_counts('sentiment'))"
      ],
      "metadata": {
        "id": "Hf-AlRDsP3Eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting data into train and test"
      ],
      "metadata": {
        "id": "uA0zUNvoP5kG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df_review_bal, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "ogonTBR-P7KW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, train_y = train['review'], train['sentiment']\n",
        "test_x, test_y = test['review'], test['sentiment']"
      ],
      "metadata": {
        "id": "cyNzCQZKP9_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_y.value_counts()"
      ],
      "metadata": {
        "id": "4MWiwPvLQAvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Representation (Bag of Words)\n",
        "The classifiers and learning algorithms expect numerical feature vectors rather than raw text documents. We need to convert the text to a more manageable representation.\n",
        "\n"
      ],
      "metadata": {
        "id": "LNZyXuDRQGhu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Count Vectorizer"
      ],
      "metadata": {
        "id": "YGvNtTOHQIly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "text = [\"I love writing code in Python. I love Python code\",\n",
        "        \"I hate writing code in Java. I hate Java code\"]\n",
        "\n",
        "df = pd.DataFrame({'review': ['review1', 'review2'], 'text':text})\n",
        "cv = CountVectorizer(stop_words='english')\n",
        "cv_matrix = cv.fit_transform(df['text'])\n",
        "df_dtm = pd.DataFrame(cv_matrix.toarray(), index=df['review'].values, columns=cv.get_feature_names())\n",
        "df_dtm"
      ],
      "metadata": {
        "id": "B1gc1r5XQNaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tfidf"
      ],
      "metadata": {
        "id": "2lfSSlW7QRTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "text = [\"I love writing code in Python. I love Python code\",\n",
        "        \"I hate writing code in Java. I hate Java code\"]\n",
        "\n",
        "df = pd.DataFrame({'review': ['review1', 'review2'], 'text':text})\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = tfidf.fit_transform(df['text'])\n",
        "df_dtm = pd.DataFrame(tfidf_matrix.toarray(), index=df['review'].values, columns=tfidf.get_feature_names())\n",
        "df_dtm"
      ],
      "metadata": {
        "id": "WPKkVaF0QUEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Turning our text data into numerical vectors"
      ],
      "metadata": {
        "id": "mbbTXZNUQWxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "train_x_vector = tfidf.fit_transform(train_x)\n",
        "# also fit the test_x_vector\n",
        "test_x_vector = tfidf.transform(test_x)\n",
        "\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# cv = CountVectorizer(stop_words='english')\n",
        "# train_x_vector = cv.fit_transform(train_x)\n",
        "# test_x_vector = cv.transform(test_x)"
      ],
      "metadata": {
        "id": "EFFqhvskQare"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x_vector"
      ],
      "metadata": {
        "id": "YEOO_IeoQeZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Types of matrix\n",
        "\n",
        "* Sparse matrix\n",
        "* Dense matrix\n"
      ],
      "metadata": {
        "id": "9E8T9TDkQk1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame.sparse.from_spmatrix(train_x_vector,\n",
        "                                  index=train_x.index,\n",
        "                                  columns=tfidf.get_feature_names())"
      ],
      "metadata": {
        "id": "Apih5q4UQtrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Selection\n",
        "ML algorithms\n",
        "\n",
        "1 Supervised learning: Regression (numerical output), Classification (discrete output)\n",
        "* Input: Review\n",
        "* Output: Sentiment (discrete)\n",
        "2 Unsupervised learning\n",
        "\n",
        "#Support Vector Machines (SVM)"
      ],
      "metadata": {
        "id": "aJZHOLL1Q0KE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svc = SVC(kernel='linear')\n",
        "svc.fit(train_x_vector, train_y)\n",
        "# svc.predict(train_x_vector[0])"
      ],
      "metadata": {
        "id": "a8FjkuJPRDxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "fhsjo9dsREj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(svc.predict(tfidf.transform(['A good movie'])))\n",
        "print(svc.predict(tfidf.transform(['An excellent movie'])))\n",
        "print(svc.predict(tfidf.transform(['\"I did not like this movie at all I gave this movie away\"'])))"
      ],
      "metadata": {
        "id": "b12QnEcPRI-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Decision Tree"
      ],
      "metadata": {
        "id": "owPMUTBJRM1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "dec_tree = DecisionTreeClassifier()\n",
        "dec_tree.fit(train_x_vector, train_y)"
      ],
      "metadata": {
        "id": "gwhb-qDhRRrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Naive Bayes"
      ],
      "metadata": {
        "id": "MhESwvwfRUcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(train_x_vector.toarray(), train_y)"
      ],
      "metadata": {
        "id": "ermBtPBDRXZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic Regression"
      ],
      "metadata": {
        "id": "q-goam7cRblr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(train_x_vector, train_y)"
      ],
      "metadata": {
        "id": "FpBmgT-DRgN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Evaluation\n",
        "##Mean Accuracy\n",
        "Return the mean accuracy on the given test data and labels."
      ],
      "metadata": {
        "id": "S1_IfHNDRioj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(svc.score(test_x_vector, test_y))\n",
        "print(dec_tree.score(test_x_vector, test_y))\n",
        "print(gnb.score(test_x_vector.toarray(), test_y))\n",
        "print(log_reg.score(test_x_vector, test_y))\n",
        "\n",
        "# svc.score('Test samples', 'True labels')"
      ],
      "metadata": {
        "id": "UUv2bUChRra9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#F1 Score\n",
        "F1 Score is the weighted average of Precision and Recall. Accuracy is used when the True Positives and True negatives are more important while F1-score is used when the False Negatives and False Positives are crucial. Also, F1 takes into account how the data is distributed, so it's useful when you have data with imbalance classes.\n",
        "\n",
        "F1 Score = 2*(Recall * Precision) / (Recall + Precision)"
      ],
      "metadata": {
        "id": "TEmsU0Y9RvWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1_score(test_y, svc.predict(test_x_vector),\n",
        "         labels=['positive', 'negative'],\n",
        "         average=None)\n",
        "\n",
        "# f1_score(y_true, y_pred, average=None)"
      ],
      "metadata": {
        "id": "ALrp5QFARxdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Classification report\n",
        "Build a text report showing the main classification metrics."
      ],
      "metadata": {
        "id": "qaDPjg3lRz6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(test_y,\n",
        "                            svc.predict(test_x_vector),\n",
        "                            labels=['positive', 'negative']))\n",
        "# classification_report(y_true, y_pred)"
      ],
      "metadata": {
        "id": "DATbww8_R4K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Confusion Matrix\n",
        "A confusion matrix) is a table with two rows and two columns that reports the number of false positives, false negatives, true positives, and true negatives"
      ],
      "metadata": {
        "id": "vHuqHo7zR7uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "conf_mat = confusion_matrix(test_y,\n",
        "                            svc.predict(test_x_vector),\n",
        "                            labels=['positive', 'negative'])\n",
        "conf_mat"
      ],
      "metadata": {
        "id": "s3hjLFUBR_Fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tuning the Model\n",
        "##GridSearchCV"
      ],
      "metadata": {
        "id": "13fVFBxuSCGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "parameters = {'C': [1,4,8,16,32] ,'kernel':['linear', 'rbf']}\n",
        "svc = SVC()\n",
        "svc_grid = GridSearchCV(svc,parameters, cv=5,)\n",
        "#              refit=True, verbose=0)\n",
        "svc_grid.fit(train_x_vector, train_y)"
      ],
      "metadata": {
        "id": "QfGMZOXuSHo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(svc_grid.best_params_)\n",
        "print(svc_grid.best_estimator_)"
      ],
      "metadata": {
        "id": "EBIyRX4zSKSg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}